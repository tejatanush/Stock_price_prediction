{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPAd9udwCT7aTGJCLEf84xu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejatanush/Stock_price_prediction/blob/main/Stock_price_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Description:**"
      ],
      "metadata": {
        "id": "myxMk5j_pXyJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Developed a model for stock price prediction, leveraging time series data to capture temporal dependencies and forecast future prices. The model utilizes historical stock prices to train a deep learning network, ensuring accurate trend predictions and informed investment strategies."
      ],
      "metadata": {
        "id": "ayqEab0ipdmy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Import Libraries"
      ],
      "metadata": {
        "id": "njOeAIo-oXi-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRxHZLrd8FTR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Import Dataset"
      ],
      "metadata": {
        "id": "Y0gK862yogWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset was imported from kaggel website. It consists of many independent features on which stock price is is predicted accurately.\n",
        "# Reference:\n",
        "https://www.kaggle.com/datasets/jainilcoder/netflix-stock-price-prediction"
      ],
      "metadata": {
        "id": "lvMWEQC_qnUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"NFLX.csv\")\n",
        "df['Date']=pd.to_datetime(df['Date'])"
      ],
      "metadata": {
        "id": "Ke-lW6EZ8QrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the columns in our dataset and make them to an array"
      ],
      "metadata": {
        "id": "HHs4zzKVsvz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data=df[['Date','Open','High','Low','Close']].values"
      ],
      "metadata": {
        "id": "6NQFmRwd9d80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Split into Training and Test set"
      ],
      "metadata": {
        "id": "KjZrjmzNooVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_size=int(len(data)*0.8)\n",
        "train_data,test_data=data[:train_size],data[train_size:]\n",
        "test_dates=df['Date'][train_size:]"
      ],
      "metadata": {
        "id": "cOgZcJDW8y7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.Feature Scaling"
      ],
      "metadata": {
        "id": "xqIdn4WDs-5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "sc=MinMaxScaler(feature_range=(0,1))\n",
        "scaled_train_data=sc.fit_transform(train_data[:,1:])\n",
        "scaled_test_data=sc.transform(test_data[:,1:])"
      ],
      "metadata": {
        "id": "p9Nq9j2A96Go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.Create Dataset\n",
        "Dataset should be created in a sequence manner so that these data can be act as an input for recurrent nueral network with a timestep of 60."
      ],
      "metadata": {
        "id": "MQmIobBetTqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(data,time_step=1):\n",
        "  X,Y=[],[]\n",
        "  for i in range(len(data)-time_step-1):\n",
        "    X.append(data[i:(i+time_step)])\n",
        "    Y.append(data[i+time_step])\n",
        "  return np.array(X),np.array(Y)\n",
        "\n",
        "time_step=60\n",
        "X_train,Y_train=create_dataset(scaled_train_data,time_step)\n",
        "X_test,Y_test=create_dataset(scaled_test_data,time_step)"
      ],
      "metadata": {
        "id": "n7_vcyou_FrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.Reshape X values in correct format"
      ],
      "metadata": {
        "id": "7HF3AzybuEbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=X_train.reshape(X_train.shape[0],X_train.shape[1],X_train.shape[2])\n",
        "X_test=X_test.reshape(X_test.shape[0],X_test.shape[1],X_test.shape[2])"
      ],
      "metadata": {
        "id": "kaZDl5IQCEI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. Build a Model"
      ],
      "metadata": {
        "id": "-Jro3MdXvYwZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create model:** To predict stock price we need to use recurrent nueral networks. So that we can use LSTM which have a good structure.LSTM-Long Short term memory. To maintain and remember all the historic values LSTM layer is good. At last we need to predict the price so we can use dense layer."
      ],
      "metadata": {
        "id": "zjLQPldZviRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,LSTM\n",
        "model=Sequential()\n",
        "model.add(LSTM(units=50,return_sequences=True,input_shape=(time_step,4)))\n",
        "model.add(LSTM(units=50,return_sequences=False))\n",
        "model.add(Dense(4))"
      ],
      "metadata": {
        "id": "KemFvW0tCqrz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67dd497c-8f2c-4336-dd17-a68fbb236ff3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compile Model:** We can use Adam as optimizer and mean_squared_error as loss function"
      ],
      "metadata": {
        "id": "BPEyKyuKw5Ik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',loss='mean_squared_error')"
      ],
      "metadata": {
        "id": "C5NBjQfQxKqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fit Mode:** Fit the model with X_train,Y_train and validation data as X_test and Y_test. Train the model upto 10 epochs"
      ],
      "metadata": {
        "id": "Vx6HBLeUxLSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train,Y_train,validation_data=(X_test,Y_test),epochs=10,batch_size=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GImxqSODPnJ",
        "outputId": "2944204c-1668-45c7-8cb9-7514869d4ae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m746/746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - loss: 0.0199 - val_loss: 0.0118\n",
            "Epoch 2/10\n",
            "\u001b[1m746/746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - loss: 0.0031 - val_loss: 0.0096\n",
            "Epoch 3/10\n",
            "\u001b[1m746/746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0023 - val_loss: 0.0046\n",
            "Epoch 4/10\n",
            "\u001b[1m746/746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.0015 - val_loss: 0.0077\n",
            "Epoch 5/10\n",
            "\u001b[1m746/746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.0012 - val_loss: 0.0036\n",
            "Epoch 6/10\n",
            "\u001b[1m746/746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.0010 - val_loss: 0.0021\n",
            "Epoch 7/10\n",
            "\u001b[1m746/746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - loss: 9.1783e-04 - val_loss: 0.0023\n",
            "Epoch 8/10\n",
            "\u001b[1m746/746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 9.3708e-04 - val_loss: 0.0031\n",
            "Epoch 9/10\n",
            "\u001b[1m746/746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.0013 - val_loss: 0.0038\n",
            "Epoch 10/10\n",
            "\u001b[1m746/746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 8.7402e-04 - val_loss: 0.0028\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a814168c580>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that our trained with less loss including validation data"
      ],
      "metadata": {
        "id": "KaWbFXP_xdl4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.Prediction"
      ],
      "metadata": {
        "id": "7xNTtjonyQ1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predict X_test and store them in test_predict. It is compulsory to inverse transform predicted data as they are not real values...they are normalized values"
      ],
      "metadata": {
        "id": "MlPRZvjGxqTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_predict=model.predict(X_test)\n",
        "test_predict=sc.inverse_transform(test_predict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqYwQihpF2LN",
        "outputId": "c238ac43-44dc-486f-8eaf-3ee3833a25da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make Y_test to y_test because Y_test is in normalized form......so make them to nuetral"
      ],
      "metadata": {
        "id": "qlSHREsxyaeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_test=sc.inverse_transform(Y_test)"
      ],
      "metadata": {
        "id": "nsGZ0iyNH2j3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dates=test_dates[time_step+1:].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "rCuYYgNMDokq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9.Accuracy"
      ],
      "metadata": {
        "id": "Rzaa43wzyxTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "r2_score(y_test,test_predict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpyQ71peH-0Q",
        "outputId": "105e6028-7038-4fb2-cf92-20aec6cee4ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9369780949639537"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have got the good results with an accuracy of 93.69%."
      ],
      "metadata": {
        "id": "u2fVQ0l-y1Bl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Final Dataframe\n",
        "Make a dataframe to compare all predicted and actual prices"
      ],
      "metadata": {
        "id": "B99LhpGXy6mx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_df=pd.DataFrame({'Date':test_dates,'Actual_open':y_test[:,0],\n",
        "                        'Actual_high':y_test[:,1],\n",
        "                        'Actual_low':y_test[:,2],\n",
        "                        'Actual_close':y_test[:,3],\n",
        "                        'Predicted_open':test_predict[:,0],\n",
        "                        'Predicted_high':test_predict[:,1],\n",
        "                        'Predicted_low':test_predict[:,2],\n",
        "                        'Predicted_close':test_predict[:,3]})\n",
        "print(result_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dq0UhJQq5T_q",
        "outputId": "80ae8c19-94bb-4881-a9cd-617e1e346152"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          Date  Actual_open  Actual_high  Actual_low  Actual_close  \\\n",
            "0   2021-07-19   541.809998   544.059998  527.049988    530.309998   \n",
            "1   2021-07-20   526.049988   534.909973  522.239990    532.280029   \n",
            "2   2021-07-21   526.070007   536.640015  520.299988    531.049988   \n",
            "3   2021-07-22   526.130005   530.989990  505.609985    513.630005   \n",
            "4   2021-07-23   510.209991   513.679993  507.000000    511.769989   \n",
            "..         ...          ...          ...         ...           ...   \n",
            "136 2022-01-31   386.760010   387.000000  372.079987    384.359985   \n",
            "137 2022-02-01   401.970001   427.700012  398.200012    427.140015   \n",
            "138 2022-02-02   432.959991   458.480011  425.540009    457.130005   \n",
            "139 2022-02-03   448.250000   451.980011  426.480011    429.480011   \n",
            "140 2022-02-04   421.440002   429.260010  404.279999    405.600006   \n",
            "\n",
            "     Predicted_open  Predicted_high  Predicted_low  Predicted_close  \n",
            "0        550.912964      551.247192     538.386658       551.241699  \n",
            "1        538.518738      539.950378     526.914917       540.074219  \n",
            "2        538.348999      539.610718     526.782043       539.967773  \n",
            "3        536.789307      538.081360     525.353455       538.515564  \n",
            "4        521.899719      524.406616     511.499542       524.899231  \n",
            "..              ...             ...            ...              ...  \n",
            "136      399.141052      404.656189     390.237366       403.768829  \n",
            "137      393.170654      398.740936     384.897156       398.666870  \n",
            "138      433.163818      435.122253     422.776489       435.274384  \n",
            "139      463.145996      462.824493     450.897095       463.598145  \n",
            "140      442.975189      444.983490     432.157379       445.814026  \n",
            "\n",
            "[141 rows x 9 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that predicted and actual values are very close to each other."
      ],
      "metadata": {
        "id": "2dUS7N-20GlI"
      }
    }
  ]
}